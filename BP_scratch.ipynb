{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single layer nerual network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15.26</td>\n",
       "      <td>14.84</td>\n",
       "      <td>0.8710</td>\n",
       "      <td>5.763</td>\n",
       "      <td>3.312</td>\n",
       "      <td>2.221</td>\n",
       "      <td>5.220</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.88</td>\n",
       "      <td>14.57</td>\n",
       "      <td>0.8811</td>\n",
       "      <td>5.554</td>\n",
       "      <td>3.333</td>\n",
       "      <td>1.018</td>\n",
       "      <td>4.956</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14.29</td>\n",
       "      <td>14.09</td>\n",
       "      <td>0.9050</td>\n",
       "      <td>5.291</td>\n",
       "      <td>3.337</td>\n",
       "      <td>2.699</td>\n",
       "      <td>4.825</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.84</td>\n",
       "      <td>13.94</td>\n",
       "      <td>0.8955</td>\n",
       "      <td>5.324</td>\n",
       "      <td>3.379</td>\n",
       "      <td>2.259</td>\n",
       "      <td>4.805</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16.14</td>\n",
       "      <td>14.99</td>\n",
       "      <td>0.9034</td>\n",
       "      <td>5.658</td>\n",
       "      <td>3.562</td>\n",
       "      <td>1.355</td>\n",
       "      <td>5.175</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0      1       2      3      4      5      6  7\n",
       "0  15.26  14.84  0.8710  5.763  3.312  2.221  5.220  1\n",
       "1  14.88  14.57  0.8811  5.554  3.333  1.018  4.956  1\n",
       "2  14.29  14.09  0.9050  5.291  3.337  2.699  4.825  1\n",
       "3  13.84  13.94  0.8955  5.324  3.379  2.259  4.805  1\n",
       "4  16.14  14.99  0.9034  5.658  3.562  1.355  5.175  1"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('seeds_dataset.csv', header = None)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Int64Index([0, 1, 2, 3, 4, 5, 6, 7], dtype='int64')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.columns)\n",
    "df.iloc[:, 7].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['0', '1', '2', '3', '4', '5', '6', '7'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# convert column names from integer to string \n",
    "\n",
    "df.columns = df.columns.map(str)\n",
    "print (df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['7'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialzied the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_hidden: number of neurons in each hidden layer and each each hidden layer has n_input + 1 weights \n",
    "# n_outputs: number of neurons in the output layer and output layer has n_hidden + 1 weights \n",
    "\n",
    "\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    # weight matrix {num_row, num_col}\n",
    "    hidden_layer = [{'weights':[random() for i in range(n_inputs + 1)]} for i in range(n_hidden)]\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    output_layer = [{'weights': [random() for i in range(n_hidden + 1)]} for i in range(n_outputs)]\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n",
      "==\n",
      "{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}\n",
      "{'weights': [0.2550690257394217, 0.49543508709194095]}\n",
      "{'weights': [0.4494910647887381, 0.651592972722763]}\n"
     ]
    }
   ],
   "source": [
    "from random import seed\n",
    "from random import random\n",
    "\n",
    "seed(1)\n",
    "# test initialize_network - two dimensional input data, one hidden layer with one neuron, binary output data \n",
    "network = initialize_network(2, 1, 2)\n",
    "\n",
    "for layer in network:\n",
    "    print (layer)\n",
    "    \n",
    "print ('==')\n",
    "\n",
    "for layer in network:\n",
    "    for neuron in layer:\n",
    "        print (neuron)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forward propagation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear weight function -- sum(weight_i * input_i) + bias\n",
    "\n",
    "def activate(weights, inputs): \n",
    "    # assume bias is the last entry in the weightvector\n",
    "    activation = weights[-1]\n",
    "    \n",
    "    for i in range(len(weights) - 1):\n",
    "        # inputs are row data (with each data point from each attribute) in the training data - elementwise multiplicaiotn\n",
    "        activation += weights[i] * inputs[i]\n",
    "        \n",
    "    return activation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use sigmoid function as activation function to transfer the linear weight function to a range between(0, 1)\n",
    "# g(z) = 1/(1 + exp(-z)) where z is the linear weight function defined in the above cell as activate(-, -)\n",
    "\n",
    "from math import exp\n",
    "\n",
    "def transfer (activation):\n",
    "    return 1/(1 + exp(-activation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "forward propagate each row in the dataset through the netwrok and \n",
    "\n",
    "save the forward propagate result in each neuron as 'output' after the neuron's weight vector and\n",
    "\n",
    "return those activated weighted inputs through the entire netwrok, \n",
    "\n",
    "which is also the model prediction for a given row of data \n",
    "\n",
    "(haven't use softmax function on output layer yet, predicted probabilities won't sum to one)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    \n",
    "    for layer in network:\n",
    "        new_inputs = []\n",
    "        for neuron in layer:\n",
    "            # eadch neruon's outputs are weighted and eprocessd by the neurons in the next layer \n",
    "            activation = activate(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation)\n",
    "            new_inputs.append(neuron['output'])  \n",
    "        # to use out/activated weighted inputs from the previous layer as input for the next layer \n",
    "        inputs = new_inputs\n",
    "#         print(inputs)\n",
    "        \n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7105668883115941\n",
      "0.6629970129852887\n"
     ]
    }
   ],
   "source": [
    "# forward propagation calculation \n",
    "# print (transfer(0.13436424411240122 + 0.763774618976614 ))\n",
    "# print (transfer(0.7105668883115941 * 0.2550690257394217 + 0.49543508709194095 ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}], [{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]]\n",
      "--\n",
      "[0.6629970129852887, 0.7253160725279748]\n",
      "--\n",
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'output': 0.7105668883115941}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095], 'output': 0.6629970129852887}, {'weights': [0.4494910647887381, 0.651592972722763], 'output': 0.7253160725279748}]\n"
     ]
    }
   ],
   "source": [
    "# test forward propagate\n",
    "\n",
    "# one example row of data\n",
    "row = [1, 0, None]\n",
    "\n",
    "seed(1)\n",
    "network = initialize_network(2, 1, 2)\n",
    "print(network)\n",
    "print ('--')\n",
    "output = forward_propagate(network, row)\n",
    "print (output)\n",
    "print ('--')\n",
    "for layer in network:\n",
    "    print (layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://www.bogotobogo.com/python/scikit-learn/Artificial-Neural-Network-ANN-4-Backpropagation.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# derivative of sigmoid function folows dg(z)/ d(z) = g(z) (1 - g(z)) where z is the weighted input function\n",
    "\n",
    "# derivative of one neuron  \n",
    "def transfer_derivative(output):\n",
    "    return output * (1 - output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the link above, to get the gradient for weight update -\n",
    "# output layer error: delta3 = (y - yhat) * transfer_derivative(forward_propagate- output layer)\n",
    "# hidden layer error: delta2 = weight_jk * delta3 * transfer_derivative(forward_propagate - hidden layer)\n",
    "# error_k: error from the kth neuron in the output layer \n",
    "# weight_jk: weight that connects the jth neuron in the previous layer to the current neuron k, which is the kth neuron in the output layer \n",
    "\n",
    "def backward_propagate_error(network, target):\n",
    "    \n",
    "    # start from the last layer \n",
    "    for i in reversed(range(len(network))):\n",
    "        layer = network[i]\n",
    "        errors = list()\n",
    "        \n",
    "        if i == len(network) - 1: \n",
    "            for j in range(len(layer)): \n",
    "                # for each neuron in the output layer, get the list of errors (y - yhat) from all the neurons in the output layer\n",
    "                neuron = layer[j]\n",
    "                errors.append(target[j] - neuron['output'])\n",
    "                                \n",
    "        else:\n",
    "            for j in range(len(layer)):\n",
    "                error = 0\n",
    "                # delta2 = error of each neuron in the output layer/delta3 weighted by the output layer weight matrix/ W_out\n",
    "                for neuron in network[i + 1]:\n",
    "#                     print (('delta3 {} ').format(neuron['delta']))\n",
    "                    \n",
    "#                     print (('W2 {} ').format(neuron['weights'][j]))\n",
    "                    error += neuron['weights'][j] * neuron['delta']\n",
    "                \n",
    "                errors.append(error)\n",
    "                \n",
    "        # first goes through ouput layer then hidden layer to update delta (changes in error)\n",
    "        for j in range(len(layer)):\n",
    "            # first run occurs when it's the output layer weight gradient - delta3 = (y - yhat) * transfer_derivative(value from forward_propagate at the corresponding layer/neuron)\n",
    "            # second run occurs when it's the output layer weight gradient - delta2 = W_out * delta3 * transfer_derivative(value from forward_propagate at the corresponding layer/neuron)\n",
    "            \n",
    "            neuron = layer[j]\n",
    "            neuron['delta'] = errors[j] * transfer_derivative(neuron['output'])\n",
    "            \n",
    "#             print (neuron['delta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--\n",
      "{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614], 'output': 0.7105668883115941, 'delta': -0.002711797799238243}\n",
      "{'weights': [0.2550690257394217, 0.49543508709194095], 'output': 0.6629970129852887, 'delta': -0.14813473120687762}\n",
      "{'weights': [0.4494910647887381, 0.651592972722763], 'output': 0.7253160725279748, 'delta': 0.05472601157879688}\n"
     ]
    }
   ],
   "source": [
    "# test back propagation\n",
    "\n",
    "target = [0, 1]\n",
    "backward_propagate_error(network, target)\n",
    "\n",
    "print('--')\n",
    "for layer in network:\n",
    "    for neuron in layer:\n",
    "        print(neuron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7105668883115941\n",
      "0.6629970129852887\n",
      "0.7253160725279748\n",
      "--\n",
      "-0.14813473120687762\n",
      "0.05472601157879688\n",
      "--\n",
      "0.005274218131534226\n"
     ]
    }
   ],
   "source": [
    "for layer in network:\n",
    "    for neuron in layer:\n",
    "        print(neuron['output'])\n",
    "\n",
    "print ('--')\n",
    "\n",
    "# delta3 calculation double checking \n",
    "print ((0 - 0.6629970129852887) * transfer_derivative(0.6629970129852887))\n",
    "print ((1 - 0.7253160725279748) * transfer_derivative(0.7253160725279748))\n",
    "\n",
    "print ('--')\n",
    "\n",
    "# delta2 calculation double checking \n",
    "a = -0.13436424411240122 * 0.14813473120687762 \n",
    "b = 0.8474337369372327 * 0.05472601157879688\n",
    "c = transfer_derivative(0.7253160725279748)\n",
    "\n",
    "print ((a + b) * c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update network 'weights' with error using Stochastic Gradient Descent: weight = weight + learning rate * error * input\n",
    "\n",
    "def update_weights(network, row, l_rate):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[: -1]\n",
    "    \n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network [i - 1]]\n",
    "        \n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                # input here is the output in each neuron from forward_propagation function\n",
    "                # dL/dW2\n",
    "                neuron['weights'][j] += l_rate * neuron['delta'] * inputs[j]\n",
    "            # bias \n",
    "            neuron['weights'][-1] += l_rate * neuron['delta']\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "\\delta_3 = (y - \\hat{y})  df(z_3)\\\\\n",
    "\\frac{\\partial{L}}{\\partial{W_2}} = a_2^T \\delta_3  \\\\\n",
    "\\frac{\\partial{L}}{\\partial{W_1}} = x^T \\delta_2\\\\\n",
    "\\delta_2 = \\delta_3 W_2^ Tdf(z_2) \\\\\n",
    "\\frac{\\partial{L}}{\\partial{b_2}} = \\delta_3\\\\\n",
    "\\frac{\\partial{L}}{\\partial{b_1}} = \\delta_2 \\\\\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(network, train, l_rate, n_epoch, n_outputs):\n",
    "\tfor epoch in range(n_epoch):\n",
    "\t\tsum_error = 0\n",
    "        \n",
    "\t\tfor row in train:\n",
    "            # forward_propagate function output is created based on the number of neurons in the output layer s\n",
    "\t\t\toutputs = forward_propagate(network, row)\n",
    "            \n",
    "            # output label vector size \n",
    "\t\t\ttarget = [0 for i in range(n_outputs)]\n",
    "            # create one hot encoding label(class 1: [0, 1], class 2:[1, 0]) \n",
    "\t\t\ttarget[row[-1]] = 1\n",
    "            \n",
    "\t\t\tsum_error += sum([(target[i] - outputs[i]) ** 2 for i in range(len(target))])\n",
    "            \n",
    "\t\t\tbackward_propagate_error(network, target)\n",
    "\t\t\tupdate_weights(network, row, l_rate)\n",
    "            \n",
    "\t\tprint('>epoch = %d, lrate = %.3f, error = %.3f' % (epoch, l_rate, sum_error)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">epoch = 0, lrate = 0.500, error = 5.947\n",
      ">epoch = 1, lrate = 0.500, error = 5.380\n",
      ">epoch = 2, lrate = 0.500, error = 5.203\n",
      ">epoch = 3, lrate = 0.500, error = 5.117\n",
      ">epoch = 4, lrate = 0.500, error = 5.028\n",
      ">epoch = 5, lrate = 0.500, error = 4.907\n",
      ">epoch = 6, lrate = 0.500, error = 4.737\n",
      ">epoch = 7, lrate = 0.500, error = 4.512\n",
      ">epoch = 8, lrate = 0.500, error = 4.237\n",
      ">epoch = 9, lrate = 0.500, error = 3.925\n",
      ">epoch = 10, lrate = 0.500, error = 3.587\n",
      ">epoch = 11, lrate = 0.500, error = 3.233\n",
      ">epoch = 12, lrate = 0.500, error = 2.874\n",
      ">epoch = 13, lrate = 0.500, error = 2.526\n",
      ">epoch = 14, lrate = 0.500, error = 2.203\n",
      ">epoch = 15, lrate = 0.500, error = 1.916\n",
      ">epoch = 16, lrate = 0.500, error = 1.666\n",
      ">epoch = 17, lrate = 0.500, error = 1.454\n",
      ">epoch = 18, lrate = 0.500, error = 1.274\n",
      ">epoch = 19, lrate = 0.500, error = 1.123\n",
      "[{'weights': [-0.7946098379053232, 0.767287431991693, 0.8048366482018935], 'output': 0.08464031263028213, 'delta': -0.006089570629683444}, {'weights': [1.245334197200103, -1.7226062225786598, -0.4789374107514859], 'output': 0.9392309331649129, 'delta': 0.008188783463631612}]\n",
      "[{'weights': [1.3791260723496621, -1.9714224883542035, 0.3389420433159421], 'output': 0.20346125597895867, 'delta': -0.03297390232442857}, {'weights': [-0.8754270587867534, 2.1116754658011763, -0.6072500699253841], 'output': 0.7801216663795356, 'delta': 0.03771613778603458}]\n"
     ]
    }
   ],
   "source": [
    "# test training \n",
    "# synthetic small dataset randoms\n",
    "\n",
    "dataset = [[2.7810836,2.550537003,0],\n",
    "\t[1.465489372,2.362125076,0],\n",
    "\t[3.396561688,4.400293529,0],\n",
    "\t[1.38807019,1.850220317,0],\n",
    "\t[3.06407232,3.005305973,0],\n",
    "\t[7.627531214,2.759262235,1],\n",
    "\t[5.332441248,2.088626775,1],\n",
    "\t[6.922596716,1.77106367,1],\n",
    "\t[8.675418651,-0.242068655,1],\n",
    "\t[7.673756466,3.508563011,1]]\n",
    "\n",
    "# for mdoel parameter initialization\n",
    "n_inputs = len(dataset[0]) - 1\n",
    "n_outputs = len(set([row[-1] for row in dataset]))\n",
    "\n",
    "network = initialize_network(n_inputs, 2, n_outputs)\n",
    "train_network(network, dataset, 0.5, 20, n_outputs)\n",
    "\n",
    "for layer in network:\n",
    "    print(layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make prediction with the network\n",
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target = 0, Got = 0\n",
      "Target = 0, Got = 0\n",
      "Target = 0, Got = 0\n",
      "Target = 0, Got = 0\n",
      "Target = 0, Got = 0\n",
      "Target = 1, Got = 1\n",
      "Target = 1, Got = 1\n",
      "Target = 1, Got = 1\n",
      "Target = 1, Got = 1\n",
      "Target = 1, Got = 1\n"
     ]
    }
   ],
   "source": [
    "# on the same small dataset used before\n",
    "\n",
    "for row in dataset:\n",
    "\tprediction = predict(network, row)\n",
    "\tprint('Target = %d, Got = %d' % (row[-1], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
