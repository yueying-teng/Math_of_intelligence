{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "scrape list of S&P500 companies from wikipedia using python 3\n",
    "Wikpedia page: https://en.wikipedia.org/wiki/List_of_S%26P_500_companies \n",
    "Generates 2 main output lists: \n",
    "1. sp_current_companies\n",
    "general format: \n",
    "[['Ticker Symbol', 'Company Name', 'GICS Sector', 'GICS Sub Indudtry']]\n",
    "example element: \n",
    "[['MMM', '3M Company', 'Industrials', 'Industrial Conglomerates']]\n",
    "2. sp_company_changes\n",
    "general format: \n",
    "[[['start_date', 'end_date'], \n",
    "['added_co1', 'added_co2', ...], \n",
    "['removed_co1', removed_co2, ...], \n",
    "[List of Ticker Symbols Active in Date Range]]]\n",
    "example element: \n",
    "[[['2017-06-19', '2017-07-16'], \n",
    "['HLT', 'ALGN', 'ANSS', 'RE'], \n",
    "['YHOO', 'TDC', 'R', 'MJN'], \n",
    "['A', 'AAL', 'AAP', 'AAPL', 'ABBV', ...]]]\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lxml import etree\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a page with request \n",
    "# response.text reads all the content sent back by the web server (and raises an error if the request was unsuccessful)\n",
    "\n",
    "r = requests.get(\n",
    "        'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies')\n",
    "\n",
    "# there are 2 main tables to scrape, but both have the same name\n",
    "# so grab each block of html table text separately \n",
    "\n",
    "# '<table class=\"wikitable sortable\">' and '</table>' are the opening and ending tags of the desired table content\n",
    "sp_co_start = r.text.index('<table class=\"wikitable sortable\">')\n",
    "sp_co_end = r.text.index('</table>')\n",
    "sp_co_text = r.text[sp_co_start: sp_co_end]\n",
    "\n",
    "sp_chg_start =  r.text.index('<table class=\"wikitable sortable\">', sp_co_end)\n",
    "sp_chg_end = r.text.index('</table>', sp_chg_start) \n",
    "sp_chg_text = r.text[sp_chg_start: sp_chg_end]\n",
    "\n",
    "# del r "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<tr>'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_co_text.splitlines()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over HTML text and extract the table rows into list elements for parsing later\n",
    "\n",
    "trList = []\n",
    "startpos = sp_co_text.index('<tr>')\n",
    "endpos = sp_co_text.index('</tr>')\n",
    "\n",
    "while startpos < len(sp_co_text):\n",
    "    tempList = []\n",
    "    searchText = sp_co_text[startpos: endpos]\n",
    "    for line in searchText.splitlines():\n",
    "        tempList.append(line)\n",
    "    trList.append(tempList[1:])\n",
    "    \n",
    "    try:\n",
    "        startpos = sp_co_text.index('<tr>', endpos)\n",
    "    except ValueError:\n",
    "        break\n",
    "    try:\n",
    "        endpos = sp_co_text.index('</tr>', startpos)\n",
    "    except ValueError:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# parse headers into list\n",
    "\n",
    "headerList = ['Ticker_symbol', 'Security', \n",
    "              'Global_Industry_Classification_Standard', 'GICS Sub Industry']\n",
    "headerColumn = []\n",
    "\n",
    "# get table columns of each header (in case wikipedia changes the layout)\n",
    "# column names ae in the first row of table just parsed, i.e. trList[0]\n",
    "for i in headerList:\n",
    "    for j in range(len(trList[0])-1):\n",
    "        if i in trList[0][j]: \n",
    "            headerColumn.insert(j, j)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 3, 4]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "headerColumn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# build list of lists with each of the 4 data elements in headerList\n",
    "\n",
    "sp_current_companies = []\n",
    "\n",
    "for table_row in trList[1:]:\n",
    "    row_list = []\n",
    "    \n",
    "    # extract ticker symbols\n",
    "    ticker_string = etree.HTML(table_row[headerColumn[0]])\n",
    "    ticker_symbol = ticker_string.xpath('//a/text()')\n",
    "    row_list.append(ticker_symbol[0])\n",
    "    \n",
    "    # extract security names\n",
    "    security_string = etree.HTML(table_row[headerColumn[1]])\n",
    "    security_symbol = security_string.xpath('//a/text()')\n",
    "    row_list.append(security_symbol[0])\n",
    "    \n",
    "    # extract GICS category\n",
    "    GICS_string = etree.HTML(table_row[headerColumn[2]])\n",
    "    GICS_symbol = GICS_string.xpath('//td/text()')\n",
    "    row_list.append(GICS_symbol[0])\n",
    "    \n",
    "    # exrtract GICS sub category\n",
    "    GICS_sub_string = etree.HTML(table_row[headerColumn[3]])\n",
    "    GICS_sub_symbol = GICS_sub_string.xpath('//td/text()')\n",
    "    row_list.append(GICS_sub_symbol[0])\n",
    "    \n",
    "    sp_current_companies.append(row_list)    \n",
    "\n",
    "\n",
    "# del trList\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  the table wanted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(sp_current_companies) \n",
    "df.to_csv('GICS.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Build out historical ticker lists\n",
    "\"\"\"\n",
    "\n",
    "# loop over HTML text and extract each table row into a list element for parsing later\n",
    "chgList = []\n",
    "startpos = sp_chg_text.index('<tr>')\n",
    "endpos = sp_chg_text.index('</tr>')\n",
    "\n",
    "while startpos < len(sp_chg_text):\n",
    "    tempList = []\n",
    "    searchText = sp_chg_text[startpos:endpos]\n",
    "    for line in searchText.splitlines():\n",
    "        tempList.append(line)\n",
    "    chgList.append(tempList[1:])\n",
    "    try:\n",
    "        startpos = sp_chg_text.index('<tr>', endpos)\n",
    "    except ValueError:\n",
    "        break\n",
    "    try:\n",
    "        endpos = sp_chg_text.index('</tr>', startpos)\n",
    "    except ValueError:\n",
    "        break\n",
    "\n",
    "# first 2 elements are header rows - remove them to make parsing easier\n",
    "del chgList[:2]\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "List of S&P 500 Component Company Changes\n",
    "this code loops over the table of changes to the S&P 500 components list, \n",
    "extracts the companies coming in and out, and the date of the change\n",
    "\"\"\"\n",
    "\n",
    "sp_company_changes_dupdates = []\n",
    "i = 0 \n",
    "while i < len(chgList):\n",
    "    row_list = []\n",
    "    if chgList[i][0].find('rowspan') > -1: \n",
    "        #extract rowspan value\n",
    "        tgtSt = str(chgList[i][0]).index('rowspan')\n",
    "        tgtEnd = str(chgList[i][0]).index('\">')\n",
    "        tgtString = str(chgList[i][0])[tgtSt+9:tgtEnd]\n",
    "        new_increment = int(tgtString)\n",
    "        #retrieve data points \n",
    "        chgDate = etree.HTML(chgList[i][0]).xpath('//td/text()')\n",
    "        inCo = etree.HTML(chgList[i][1]).xpath('//td/text()')\n",
    "        outCo = etree.HTML(chgList[i][3]).xpath('//td/text()')\n",
    "        #loop over additional inner rows and retrieve values\n",
    "        for j in range(1, new_increment):\n",
    "            inCo += etree.HTML(chgList[i+j][0]).xpath('//td/text()')\n",
    "            outCo += etree.HTML(chgList[i+j][2]).xpath('//td/text()')\n",
    "        row_list.extend((chgDate, inCo, outCo))\n",
    "        i += new_increment\n",
    "    elif chgList[i][0].find('rowspan') == -1:\n",
    "        #retrieve data points \n",
    "        chgDate = etree.HTML(chgList[i][0]).xpath('//td/text()')\n",
    "        inCo = etree.HTML(chgList[i][1]).xpath('//td/text()')\n",
    "        outCo = etree.HTML(chgList[i][3]).xpath('//td/text()')\n",
    "        row_list.extend((chgDate, inCo, outCo))\n",
    "        i += 1\n",
    "    sp_company_changes_dupdates.append(row_list)\n",
    "    \n",
    "del chgList\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "in order to account for dates with multiple entries, the following\n",
    "very inefficient code was added, but at least it works\n",
    "\"\"\"\n",
    "\n",
    "#get list of unique change dates \n",
    "sp_company_changes = []\n",
    "for i in sp_company_changes_dupdates: \n",
    "    if [i[0]] not in sp_company_changes:\n",
    "        sp_company_changes.append([i[0]])\n",
    "\n",
    "#super inefficient merge of the 2 lists to consolidate all component changes  \n",
    "# for each date into one entry\n",
    "for dt in sp_company_changes:    \n",
    "    dt.append([])\n",
    "    dt.append([])\n",
    "    for i in sp_company_changes_dupdates:\n",
    "        if i[0] == dt[0]:\n",
    "            dt[1].extend(i[1])\n",
    "            dt[2].extend(i[2])\n",
    "\n",
    "\n",
    "# create start and end dates for each company change entry\n",
    "dateHolder = ''\n",
    "for idx, val in enumerate(sp_company_changes):\n",
    "    inputDate = datetime.strptime(val[0][0], '%B %d, %Y')\n",
    "    startDate = datetime.strftime(inputDate, '%Y-%m-%d')\n",
    "    if idx == 0: \n",
    "        if inputDate > datetime.today(): \n",
    "            endDate = datetime.strftime(inputDate, '%Y-%m-%d')\n",
    "        elif inputDate <= datetime.today():\n",
    "            endDate = datetime.today().strftime('%Y-%m-%d')\n",
    "    elif idx > 0:\n",
    "        endDate = datetime.strftime(dateHolder - timedelta(days=1), '%Y-%m-%d')\n",
    "    val[0][0] = startDate\n",
    "    val[0].append(endDate)    \n",
    "    dateHolder = inputDate\n",
    "\n",
    "\n",
    "# Wikipedia's ticker \"UA-C\" is actually \"UA\" so need to find and replace it\n",
    "for i in range(len(sp_company_changes)):\n",
    "    for j in range(len(sp_company_changes[i][1])):\n",
    "        if sp_company_changes[i][1][j] == 'UA-C': \n",
    "            holder = sp_company_changes[i][1][j]\n",
    "            sp_company_changes[i][1][j] = holder.replace(\"-C\", '')\n",
    "    for k in range(len(sp_company_changes[i][2])):\n",
    "        if sp_company_changes[i][2][k] == 'UA-C': \n",
    "            holder = sp_company_changes[i][2][k]\n",
    "            sp_company_changes[i][2][k] = holder.replace(\"-C\", '')\n",
    "    \n",
    "\n",
    "#put current tickers into their own list\n",
    "currentTickers = []\n",
    "for i in sp_current_companies: \n",
    "    currentTickers.append(i[0])\n",
    "\n",
    "\n",
    "#create list of tickers that was active during each date range, starting\n",
    "# with the current ticker list\n",
    "add_list, remove_list, ticker_list = [], [], []\n",
    "for i in sp_company_changes:\n",
    "    if i[0][0] > datetime.today().strftime('%Y-%m-%d'):\n",
    "        i.append([])\n",
    "    elif i[0][0] <= datetime.today().strftime('%Y-%m-%d') <= i[0][1]:\n",
    "        i.append([])\n",
    "        for a in currentTickers: \n",
    "            i[3].append(a)\n",
    "        for j in i[1]: \n",
    "            remove_list.append(j)\n",
    "        for k in i[2]: \n",
    "            add_list.append(k)\n",
    "        for ticker in i[3]: \n",
    "            ticker_list.append(ticker)\n",
    "        i[3].sort()\n",
    "    elif i[0][0] <= i[0][1] < datetime.today().strftime('%Y-%m-%d'):\n",
    "        for j in remove_list:\n",
    "            if j in ticker_list: \n",
    "                ticker_list.remove(j)\n",
    "            else: \n",
    "                continue\n",
    "        for k in add_list:\n",
    "            ticker_list.append(k)\n",
    "        i.append([])\n",
    "        for a in ticker_list:\n",
    "            i[3].append(a)\n",
    "        i[3].sort()\n",
    "        add_list, remove_list, ticker_list = [], [], []\n",
    "        for j in i[1]: \n",
    "            remove_list.append(j)\n",
    "        for k in i[2]: \n",
    "            add_list.append(k)\n",
    "        for ticker in i[3]:\n",
    "            ticker_list.append(ticker)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
