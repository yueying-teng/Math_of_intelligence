{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything here \n",
    "\n",
    "http://people.duke.edu/~ccc14/sta-663-2017/13A_LinearAlgebra1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Matrix_decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "consider the following points when choosing the method of matrix decomposition:\n",
    "- all of them approximate the matrix in some way.\n",
    "- some may make the problem easier to solve.\n",
    "- the algorithm.\n",
    "   - Is there an efficient way to find the decomposition? (especially for large matrices. for example SVD is computed more efficiently for large matrices than eigen-decomposition.)\n",
    "   - Can we obtain it with an online algorithm? (updating factorization when new data samples comes in without recomputing all the steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some points on the reason of matrix decompostion.\n",
    "\n",
    "How do you actually solve a large system of linear equations on a computer and how do you actually diagonalize a large matrix (that theory tells you is diagonalizable) on a computer. These are very difficult problems since there is a huge gap between the theoretical results and actual computations. That gap is caused of course by rounding errors on a computer. Loads of books are written on the subject as, needless to say, it's of immense importance. Many factorizations of matrices (e.g., LU and QR) are meant to address such issues. To make computations more robust and more efficient.\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://lazyprogrammer.me/tutorial-principal-components-analysis-pca/\n",
    "\n",
    "###  PCA\n",
    "PCA finds a matrix Q that, when multiplied by the original data matrix X, returns a linearly transformed data matrix Z, where:\n",
    "\n",
    "**Z = XQ**\n",
    "\n",
    "The interesting thing about PCA is how it chooses Q.\n",
    "\n",
    "PCA reduces dimensionality by moving as much “information” as possible into as few dimensions as possible. The information here is measured by unpredictability, i.e. variance. The end result is that the transformed matrix Z has most of its variance in the first column, less variance in the second column, even less variance in the third column, etc.\n",
    "\n",
    "### Eigenvalues & Eigenvectors\n",
    "when multiply a vector by a matrix, the direction of the vector is changed.\n",
    "\n",
    "Eigenvalues λ and eigenvectors v have the property that, if multiplied by A, a matrix, is the same as multiplying the eigenvector by a constant – the eigenvalue, i.e. the eigenvector does not change direction, it only gets shorter or longer by a factor of λ. In other words:\n",
    "\n",
    "**Av = λv**\n",
    "\n",
    "where A is the matrix, v is the eigenvector and λ the eigenvalue.\n",
    "\n",
    "In the case of PCA, all the eigenvectors from the empirical covarince matrix are lined upsuch that the corresponding λ, eigenvallues, are in descending order. In matrix form, the above can be expressed by:\n",
    "\n",
    "**AV = VΛ **\n",
    "\n",
    "where V is the matrix with eigenvectors as columns, which is orthonormal, and Λ is a diagonal matrix of eigenvalues.\n",
    "\n",
    "### SVD or Diagonalization in finding the eigenpairs\n",
    "\n",
    "The method from textbook – solving a polynomial to get the eigenvalues, plugging the eigenvalues into the eigenvalue equation to get the eigenvectors, etc. doesn’t really translate to computer code.\n",
    "\n",
    "**SVD**\n",
    "- Square the diagonal matrix S, and divide by sum(S) to obtain eigenvalues\n",
    "- Matrix Vt (or U) will contain the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ince ΛΛ is a diagonal matrix, there are no correlations in the transformed data. The variance of each dimension of ZZ is equal to the eigenvalues. In addition, because we sorted the eigenvalues in descending order, the first dimension of ZZ has the most variance, the second dimension has the second-most variance, etc. So most of the information is kept in the leading columns, as promised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pca = PCA()\n",
    "X = np.random.random((100,10)) # generate an N = 100, D = 10 random data matrix\n",
    "Z = pca.fit_transform(X)\n",
    "\n",
    "# visualize the covariance of Z\n",
    "plt.imshow(np.cov(Z.T))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe that the off-diagonals are 0 and that the variances are in decreasing order.\n",
    "\n",
    "You can also confirm that QTQ=IQTQ=I."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "QTQ = pca.components_.T.dot(pca.components_)\n",
    "plt.imshow(QTQ)\n",
    "plt.show()\n",
    "\n",
    "print np.around(QTQ, decimals=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
