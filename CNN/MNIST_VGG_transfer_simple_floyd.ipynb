{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/blob/master/examples/mnist_transfer_cnn.py\n",
    "\n",
    "** Transfer learning toy example**\n",
    "\n",
    "- Train a simple convnet on the MNIST dataset the first 5 digits [0...4].\n",
    "- Freeze convolutional layers and fine-tune dense/output layers for the classification for the rest digits [5...9].\n",
    "\n",
    "Get to 99.8% test accuracy after 5 epochs for the first five digits classifier and 99.2% for the last five digits after transfer + fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "import datetime\n",
    "import keras \n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K\n",
    "\n",
    "from scipy.misc import imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions \n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# number of convolutional filters to use \n",
    "filters = 32\n",
    "\n",
    "# convolutional kernel size \n",
    "kernel_size = 3\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "pool_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    # tf backend has K.image_data_format() == 'channels_last'\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train, test, num_classes):\n",
    "    \n",
    "    # change input data size so they match tensorflow requirements\n",
    "    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n",
    "    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    # normalization like \n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # one hot encode class vectors \n",
    "    y_train = keras.utils.to_categorical(train[1], num_classes)\n",
    "    y_test = keras.utils.to_categorical(test[1], num_classes)\n",
    "    \n",
    "    # compile for training\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adadelta', metrics = ['accuracy'])\n",
    "    \n",
    "    t = now()\n",
    "    model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_data = (x_test, y_test))\n",
    "    \n",
    "    print('Training time: %s' % (now() - t))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30596, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# create two datasets one with digits below 5 and the other above\n",
    "\n",
    "x_train_lt5 = x_train[y_train < 5]\n",
    "y_train_lt5 = y_train[y_train < 5]\n",
    "x_test_lt5 = x_test[y_test < 5]\n",
    "y_test_lt5 = y_test[y_test < 5]\n",
    "\n",
    "x_train_gte5 = x_train[y_train >= 5]\n",
    "y_train_gte5 = y_train[y_train >= 5] - 5\n",
    "x_test_gte5 = x_test[y_test >= 5]\n",
    "y_test_gte5 = y_test[y_test >= 5] - 5\n",
    "\n",
    "print(x_train_lt5.shape)\n",
    "print(x_train_lt5[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define two groups of layers: feature(convolutions) and classification(dense)\n",
    "\n",
    "feature_layers = [Conv2D(filters, kernel_size, padding = 'valid', input_shape = input_shape),\n",
    "                 Activation('relu'),\n",
    "                 Conv2D(filters, kernel_size),\n",
    "                 Activation('relu'),\n",
    "                 MaxPooling2D(pool_size = pool_size),\n",
    "                 Dropout(0.25),\n",
    "                 Flatten()]\n",
    "\n",
    "\n",
    "classification_layers = [Dense(128),\n",
    "                         Activation('relu'),\n",
    "                         Dropout(0.5), \n",
    "                         Dense(num_classes), \n",
    "                         Activation('softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the complete model\n",
    "\n",
    "model_fc = Sequential(feature_layers + classification_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_7 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "activation_13 (Activation)   (None, 26, 26, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 24, 24, 32)        9248      \n",
      "_________________________________________________________________\n",
      "activation_14 (Activation)   (None, 24, 24, 32)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 12, 12, 32)        0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 4608)              0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 128)               589952    \n",
      "_________________________________________________________________\n",
      "activation_15 (Activation)   (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 10)                1290      \n",
      "_________________________________________________________________\n",
      "activation_16 (Activation)   (None, 10)                0         \n",
      "=================================================================\n",
      "Total params: 600,810\n",
      "Trainable params: 600,810\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_fc.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30596, 28, 28, 1)\n",
      "30596 train samples\n",
      "5139 test samples\n",
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/5\n",
      "30596/30596 [==============================] - 53s 2ms/step - loss: 0.1744 - acc: 0.9437 - val_loss: 0.0274 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "30596/30596 [==============================] - 55s 2ms/step - loss: 0.0477 - acc: 0.9854 - val_loss: 0.0174 - val_acc: 0.9942\n",
      "Epoch 3/5\n",
      "30596/30596 [==============================] - 57s 2ms/step - loss: 0.0321 - acc: 0.9908 - val_loss: 0.0098 - val_acc: 0.9969\n",
      "Epoch 4/5\n",
      "30596/30596 [==============================] - 51s 2ms/step - loss: 0.0262 - acc: 0.9923 - val_loss: 0.0179 - val_acc: 0.9946\n",
      "Epoch 5/5\n",
      "30596/30596 [==============================] - 52s 2ms/step - loss: 0.0215 - acc: 0.9934 - val_loss: 0.0085 - val_acc: 0.9973\n",
      "Training time: 0:04:28.141054\n",
      "Test score: 0.008491175898898667\n",
      "Test accuracy: 0.9972757345787118\n"
     ]
    }
   ],
   "source": [
    "# train model for 5-digit clasification [0...4]\n",
    "\n",
    "train_model(model_fc, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freeze feature layers and rebuild the model for a different classification task [5...9]\n",
    "\n",
    "for l in feature_layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (29404, 28, 28, 1)\n",
      "29404 train samples\n",
      "4861 test samples\n",
      "Train on 29404 samples, validate on 4861 samples\n",
      "Epoch 1/5\n",
      "29404/29404 [==============================] - 21s 704us/step - loss: 0.2463 - acc: 0.9245 - val_loss: 0.0523 - val_acc: 0.9842\n",
      "Epoch 2/5\n",
      "29404/29404 [==============================] - 21s 718us/step - loss: 0.0801 - acc: 0.9764 - val_loss: 0.0385 - val_acc: 0.9872\n",
      "Epoch 3/5\n",
      "29404/29404 [==============================] - 19s 658us/step - loss: 0.0595 - acc: 0.9813 - val_loss: 0.0305 - val_acc: 0.9895\n",
      "Epoch 4/5\n",
      "29404/29404 [==============================] - 20s 665us/step - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0318 - val_acc: 0.9893\n",
      "Epoch 5/5\n",
      "29404/29404 [==============================] - 19s 644us/step - loss: 0.0467 - acc: 0.9855 - val_loss: 0.0237 - val_acc: 0.9930\n",
      "Training time: 0:01:39.823435\n",
      "Test score: 0.023744944033462122\n",
      "Test accuracy: 0.9930055544126722\n"
     ]
    }
   ],
   "source": [
    "# tranfer - train dense layers for new classification task [5...9]. training using new task data \n",
    "\n",
    "train_model(model_fc , (x_train_gte5, y_train_gte5), (x_test_gte5, y_test_gte5), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer learning with VGG and MNIST\n",
    "\n",
    "- **retrain output dense layer only/ using VGG as feature extractor(bottleneck feature)** The output layer in a vgg16 is a softmax activation with 1000 categories. Remove this layer and use the remaining model to extract features. Retrain a separated classificstion layer only using the features extracted from VGG. **cannot use data augmentation in this case**\n",
    "\n",
    "- **retrain all layers but with replaced Dense layer/ using VGG as feature extractor(bottleneck feature)** The output layer in a vgg16 is a softmax activation with 1000 categories. Remove this layer and replace it with a softmax layer of 10 categories. Retrain the modified VGG model using the features extracted from VGG without Dense layer.**can use data augmentation in this case**\n",
    "\n",
    "- **freeze the weights of the first few layers** the first few layers capture universal features like curves and edges, which are also relevant to the new problem. VGG architectue is not changed in this case, but image size is **different** from which (224 x 224) VGG is trained on. Easier to retrain on the Dense layer Repalced VGG16. - fine tuning?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 as feature extractor & feed the features to a simple linear model\n",
    "\n",
    "### VGG16 has requirement on minimium input size, which cannot be met by MNIST input without modification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrain output dense layer only\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "10469376/11490434 [==========================>...] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "57360384/58889256 [============================>.] - ETA: 0s_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load VGG16 model weights\n",
    "\n",
    "model = VGG16(weights = 'imagenet', include_top = False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check which backend is used and reshape the data to match Keras' expectation \n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat the gray scale channel to create three channels for all images before resizing them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VGG is trained on color images - repeat grayscale image over three color channels\n",
    "# np.repeat - if ``axis = 0`` it will be the first dimension and if ``axis = -1`` it will be the last dimension.\n",
    "# expand channels for both training and testing images \n",
    "\n",
    "train_color_channel = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_color_channel.append(np.repeat(x_train[i], 3, axis = -1))\n",
    "    \n",
    "test_color_channel = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_color_channel.append(np.repeat(x_test[i], 3, axis = -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change unit8 data type\n",
    "\n",
    "x_train = np.array(train_color_channel).astype('float32')\n",
    "x_test = np.array(test_color_channel).astype('float32')\n",
    "\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### resize images to meet VGG16 minimum requirement on input size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### resize does not work for images with only one color channel, but for 3 and 4 channels \n",
    "\n",
    "from skimage.transform import resize \n",
    "\n",
    "def resize (images):\n",
    "    x = np.zeros((images.shape[0], 56, 56, 3))\n",
    "    for i in range(images.shape[0]):\n",
    "        x[i] = imresize(images[i], (56, 56, 3), interp = 'bilinear', mode = None)\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 56, 56, 3)\n",
      "number of train samples:  60000\n",
      "number of test samples:  10000\n",
      "number of train sample labels:  60000\n"
     ]
    }
   ],
   "source": [
    "# resize images from (28, 28) to (56, 56)\n",
    "x_train = resize(x_train)\n",
    "x_test = resize(x_test)\n",
    "\n",
    "\n",
    "# normalization like \n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('number of train samples: ', x_train.shape[0])\n",
    "print('number of test samples: ', x_test.shape[0])\n",
    "print('number of train sample labels: ', y_train.shape[0])\n",
    "\n",
    "\n",
    "# one hot encode class vectors\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('x_train.npy', x_train)\n",
    "np.save('x_test.npy', x_test)\n",
    "np.save('y_train.npy', y_train)\n",
    "np.save('y_test.npy', y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# extract features from training and testing data using pretrained VGG16 without top \n",
    "\n",
    "now = datetime.datetime.now\n",
    "t = now()\n",
    "\n",
    "feature_train = model.predict(x_train)\n",
    "feature_test = model.predict(x_test)\n",
    "\n",
    "print('Feature extracting time: %s' % (now() - t))\n",
    "\n",
    "print(feature_train.shape)\n",
    "print(feature_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 1, 1, 512)\n",
      "(10000, 1, 1, 512)\n"
     ]
    }
   ],
   "source": [
    "# load features extracted using FloydHub\n",
    "\n",
    "feature_train = np.load('feature_train.npy')\n",
    "feature_test = np.load('feature_test.npy')\n",
    "\n",
    "print(feature_train.shape)\n",
    "print(feature_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_2 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 133,898\n",
      "Trainable params: 133,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the data without reshaping\n",
    "\n",
    "feature_train = np.load('feature_train.npy')\n",
    "feature_test = np.load('feature_test.npy')\n",
    "\n",
    "# create the simple linear model that will be fed with extracted features \n",
    "\n",
    "new_model = Sequential()\n",
    "\n",
    "# input_shape is the image/feature shape \n",
    "new_model.add(Flatten(input_shape = (1, 1, 512)))\n",
    "\n",
    "new_model.add(Dense(256, activation = 'relu', input_shape = (512, )))\n",
    "new_model.add(Dropout(0.5))\n",
    "new_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.3908 - acc: 0.8829 - val_loss: 0.1429 - val_acc: 0.9585\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.1700 - acc: 0.9471 - val_loss: 0.1123 - val_acc: 0.9674\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.1400 - acc: 0.9562 - val_loss: 0.1190 - val_acc: 0.9621\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1241 - acc: 0.9606 - val_loss: 0.0869 - val_acc: 0.9726\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 5s 91us/step - loss: 0.1138 - acc: 0.9636 - val_loss: 0.0840 - val_acc: 0.9727\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 5s 92us/step - loss: 0.1063 - acc: 0.9659 - val_loss: 0.0770 - val_acc: 0.9766\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 5s 90us/step - loss: 0.1013 - acc: 0.9674 - val_loss: 0.0772 - val_acc: 0.9751\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 5s 81us/step - loss: 0.0978 - acc: 0.9681 - val_loss: 0.0776 - val_acc: 0.9758\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 5s 86us/step - loss: 0.0921 - acc: 0.9701 - val_loss: 0.0732 - val_acc: 0.9766\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.0901 - acc: 0.9705 - val_loss: 0.0735 - val_acc: 0.9771\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 5s 76us/step - loss: 0.0875 - acc: 0.9715 - val_loss: 0.0697 - val_acc: 0.9780\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0850 - acc: 0.9725 - val_loss: 0.0689 - val_acc: 0.9772\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 5s 85us/step - loss: 0.0810 - acc: 0.9737 - val_loss: 0.0700 - val_acc: 0.9774\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 5s 88us/step - loss: 0.0822 - acc: 0.9730 - val_loss: 0.0721 - val_acc: 0.9778\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.0787 - acc: 0.9742 - val_loss: 0.0686 - val_acc: 0.9775\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 5s 79us/step - loss: 0.0779 - acc: 0.9746 - val_loss: 0.0677 - val_acc: 0.9786\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0745 - acc: 0.9754 - val_loss: 0.0686 - val_acc: 0.9776\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0746 - acc: 0.9755 - val_loss: 0.0646 - val_acc: 0.9797\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 5s 77us/step - loss: 0.0722 - acc: 0.9757 - val_loss: 0.0708 - val_acc: 0.9776\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 5s 83us/step - loss: 0.0703 - acc: 0.9770 - val_loss: 0.0642 - val_acc: 0.9810\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 6s 94us/step - loss: 0.0703 - acc: 0.9770 - val_loss: 0.0651 - val_acc: 0.9796\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.0676 - acc: 0.9774 - val_loss: 0.0644 - val_acc: 0.9790\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.0670 - acc: 0.9780 - val_loss: 0.0759 - val_acc: 0.9775\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.0668 - acc: 0.9779 - val_loss: 0.0642 - val_acc: 0.9812\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0653 - acc: 0.9782 - val_loss: 0.0629 - val_acc: 0.9808\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0635 - acc: 0.9789 - val_loss: 0.0697 - val_acc: 0.9804\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 6s 99us/step - loss: 0.0618 - acc: 0.9800 - val_loss: 0.0670 - val_acc: 0.9802\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 6s 96us/step - loss: 0.0621 - acc: 0.9795 - val_loss: 0.0650 - val_acc: 0.9807\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.0613 - acc: 0.9793 - val_loss: 0.0668 - val_acc: 0.9816\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.0616 - acc: 0.9794 - val_loss: 0.0704 - val_acc: 0.9796\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model compile \n",
    "new_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# model fitting \n",
    "# - x_train in the original model.fit has to be of (num_exampels, nrows, ncols, nchannels)\n",
    "# - now only the Dense layer is trained, x_trian's size should match this requiremnt \n",
    "\n",
    "history = new_model.fit(feature_train, y_train, batch_size = batch_size, \n",
    "                        epochs = epochs, validation_data = (feature_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 32us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07039157128121806, 0.9796]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_model.evaluate(feature_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### reshaped feature data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_12 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 133,898\n",
      "Trainable params: 133,898\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.3822 - acc: 0.8853 - val_loss: 0.1357 - val_acc: 0.9603\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 6s 95us/step - loss: 0.1662 - acc: 0.9477 - val_loss: 0.1120 - val_acc: 0.9651\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.1380 - acc: 0.9560 - val_loss: 0.0944 - val_acc: 0.9710\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.1234 - acc: 0.9609 - val_loss: 0.0917 - val_acc: 0.9720\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.1139 - acc: 0.9635 - val_loss: 0.0854 - val_acc: 0.9736\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.1055 - acc: 0.9662 - val_loss: 0.0780 - val_acc: 0.9763\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 6s 98us/step - loss: 0.1025 - acc: 0.9666 - val_loss: 0.0792 - val_acc: 0.9749\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0940 - acc: 0.9701 - val_loss: 0.0758 - val_acc: 0.9751\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0939 - acc: 0.9694 - val_loss: 0.0786 - val_acc: 0.9759\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 6s 104us/step - loss: 0.0914 - acc: 0.9698 - val_loss: 0.0781 - val_acc: 0.9757\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0868 - acc: 0.9718 - val_loss: 0.0702 - val_acc: 0.9784\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.0872 - acc: 0.9713 - val_loss: 0.0679 - val_acc: 0.9791\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.0832 - acc: 0.9731 - val_loss: 0.0739 - val_acc: 0.9782\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.0807 - acc: 0.9735 - val_loss: 0.0709 - val_acc: 0.9784\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.0796 - acc: 0.9738 - val_loss: 0.0688 - val_acc: 0.9809\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 6s 103us/step - loss: 0.0768 - acc: 0.9749 - val_loss: 0.0693 - val_acc: 0.9789\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 6s 102us/step - loss: 0.0744 - acc: 0.9754 - val_loss: 0.0684 - val_acc: 0.9803\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 6s 108us/step - loss: 0.0728 - acc: 0.9763 - val_loss: 0.0631 - val_acc: 0.9808\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0724 - acc: 0.9762 - val_loss: 0.0668 - val_acc: 0.9808\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 7s 118us/step - loss: 0.0691 - acc: 0.9771 - val_loss: 0.0716 - val_acc: 0.9786\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 6s 105us/step - loss: 0.0706 - acc: 0.9770 - val_loss: 0.0707 - val_acc: 0.9773\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 7s 112us/step - loss: 0.0674 - acc: 0.9783 - val_loss: 0.0661 - val_acc: 0.9805\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0669 - acc: 0.9774 - val_loss: 0.0683 - val_acc: 0.9798: 1s - los\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 7s 109us/step - loss: 0.0666 - acc: 0.9785 - val_loss: 0.0650 - val_acc: 0.9808\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 7s 113us/step - loss: 0.0638 - acc: 0.9792 - val_loss: 0.0657 - val_acc: 0.9810\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 7s 111us/step - loss: 0.0648 - acc: 0.9787 - val_loss: 0.0666 - val_acc: 0.9809\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 7s 108us/step - loss: 0.0614 - acc: 0.9793 - val_loss: 0.0717 - val_acc: 0.9788\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 7s 110us/step - loss: 0.0621 - acc: 0.9792 - val_loss: 0.0689 - val_acc: 0.9805\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 7s 115us/step - loss: 0.0596 - acc: 0.9800 - val_loss: 0.0630 - val_acc: 0.9824\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 6s 100us/step - loss: 0.0599 - acc: 0.9803 - val_loss: 0.0636 - val_acc: 0.9819\n"
     ]
    }
   ],
   "source": [
    "# reshaping the feature data is like flattening the input. Therefore, flatten layer is not needed again \n",
    "\n",
    "flatten_train = feature_train.reshape((feature_train.shape[0], 512))\n",
    "flatten_test = feature_test.reshape((feature_test.shape[0], 512))\n",
    "\n",
    "# create the simple linear model that will be fed with extracted features \n",
    "\n",
    "reshape_feature_model = Sequential()\n",
    "\n",
    "# input_shape is the image/feature shape \n",
    "# new_model.add(Flatten(input_shape = (1, 1, 512)))\n",
    "\n",
    "reshape_feature_model.add(Dense(256, activation = 'relu', input_shape = (512, )))\n",
    "reshape_feature_model.add(Dropout(0.5))\n",
    "reshape_feature_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "print (reshape_feature_model.summary())\n",
    "\n",
    "\n",
    "# model compile \n",
    "reshape_feature_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# model fitting \n",
    "# - x_train in the original model.fit has to be of (num_exampels, nrows, ncols, nchannels)\n",
    "# - now only the Dense layer is trained, x_trian's size should match this requiremnt \n",
    "\n",
    "history = reshape_feature_model.fit(flatten_train, y_train, batch_size = batch_size, \n",
    "                        epochs = epochs, validation_data = (flatten_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06362211907025193, 0.9819]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reshape_feature_model.evaluate(flatten_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 0s 34us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06543155931796063, 0.9812]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # save trained model weights \n",
    "\n",
    "# new_model.save_weights('vgg_feature_extractor.h5')\n",
    "\n",
    "# # load weights for model evaluation \n",
    "# new_model.load_weights('vgg_feature_extractor.h5')\n",
    "\n",
    "new_model.evaluate(feature_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### technically new_model and reshape_feature_model should have similar performance ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5,1,'Training and Testing loss')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print (history.history.keys())\n",
    "\n",
    "train_acc = history.history['acc']\n",
    "train_loss = history.history['loss']\n",
    "val_acc = history.history['val_acc']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize = (14, 5))\n",
    "fig.add_subplot(1, 2, 1)\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.plot(epochs, train_acc, 'bo', label = 'Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'b-', label = 'Validation accurcy')\n",
    "plt.legend()\n",
    "plt.title('Training and Testing accuracy')\n",
    "\n",
    "fig.add_subplot(1, 2, 2)\n",
    "epochs = range(1, len(train_acc) + 1)\n",
    "plt.plot(epochs, train_loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'b-', label = 'Validation loss')\n",
    "plt.legend()\n",
    "plt.title('Training and Testing loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 as feature extractor & feed the features to the top layer repalced VGG\n",
    "\n",
    "with data augmentation and adapting learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 56, 56, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 56, 56, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 56, 56, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 28, 28, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 28, 28, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 28, 28, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 14, 14, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 14, 14, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 14, 14, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 7, 7, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 7, 7, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 3, 3, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 3, 3, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base = VGG16(weights = 'imagenet', include_top = False, input_shape = (56, 56, 3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 10)\n",
      "(60000, 56, 56, 3)\n"
     ]
    }
   ],
   "source": [
    "# load the preprocessed MNIST data (56, 56, 3)\n",
    "\n",
    "x_train = np.load('x_train.npy')\n",
    "x_test = np.load('x_test.npy')\n",
    "y_train = np.load('y_train.npy')\n",
    "y_test = np.load('y_test.npy')\n",
    "\n",
    "print(y_train.shape)\n",
    "print(x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2570      \n",
      "=================================================================\n",
      "Total params: 14,848,586\n",
      "Trainable params: 14,848,586\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# stack the Dense layer on top of top layer removed VGG16\n",
    "\n",
    "replaced_model = Sequential()\n",
    "\n",
    "replaced_model.add(conv_base)\n",
    "replaced_model.add(Flatten())\n",
    "replaced_model.add(Dense(256, activation = 'relu'))\n",
    "replaced_model.add(Dropout(0.5))\n",
    "replaced_model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "replaced_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trainable weight matrics before freezing: 30\n",
      "number of trainable weight matrics after freezing: 4\n"
     ]
    }
   ],
   "source": [
    "print ('number of trainable weight matrics before freezing:', len(replaced_model.trainable_weights))\n",
    "\n",
    "conv_base.trainable = False\n",
    "\n",
    "print ('number of trainable weight matrics after freezing:', len(replaced_model.trainable_weights))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data augmentation & Adapting learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ImageDataGenerator - Generates batches of tensor image data with real-time data augmentation, augmented data is looped in BATCHS\n",
    "\n",
    "train_datagen = ImageDataGenerator(# rescale = 1./255,\n",
    "    rotation_range = 40,\n",
    "    width_shift_range = 0.2,\n",
    "    height_shift_range = 0.2,\n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2,\n",
    "    horizontal_flip = True,\n",
    "    fill_mode = 'nearest'\n",
    ")\n",
    "\n",
    "# testing data shouldn't be augmented\n",
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "# Compute the internal data stats related to the data-dependent transformations, based on an array of sample data. \n",
    "# Only required if featurewise_center or featurewise_std_normalization or zca_whitening.\n",
    "\n",
    "# train_datagen.fit(x_train)\n",
    "# test_datagen.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1266/1875 [===================>..........] - ETA: 101s - loss: 2.3015 - acc: 0.1125"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau, CSVLogger, EarlyStopping\n",
    "\n",
    "# define Callback functions which will be called by model during runtime when specified conditions are satisfied\n",
    "\n",
    "# ReduceLROnPlateau - Reduce learning rate when a metric has stopped improving\n",
    "lr_reducer = ReduceLROnPlateau(factor = np.sqrt(0.1), #  new_lr = lr * factor\n",
    "                               cooldown = 0, # number of epochs to wait before resuming normal operation after lr has been reduced.\n",
    "                               patience = 2, min_lr = 0.5e-6)\n",
    "csv_logger = CSVLogger('./vgg16imagenetpretrained_mnist0_data_argumentation.csv')\n",
    "early_stopper = EarlyStopping(min_delta = 0.001, patience = 10)\n",
    "\n",
    "opt = optimizers.rmsprop(lr = 0.0001, decay = 1e-6)\n",
    "\n",
    "# model compilation \n",
    "replaced_model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "\n",
    "# model fitting\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "\n",
    "# fit_generator - Fits the model on data generated batch-by-batch by a Python generator\n",
    "# train_datagen.flow - flow method takes numpy data & label arrays and generates batches of augmented/normalized data.\n",
    "history = replaced_model.fit_generator(train_datagen.flow(x_train, y_train, batch_size = batch_size), \n",
    "                             steps_per_epoch = len(x_train)// batch_size, \n",
    "                                       epochs = epochs, validation_data = (x_test, y_test),\n",
    "                                      callbacks = [lr_reducer, early_stopper, csv_logger])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "replaced_model.save_weights(\"mnist_adam_vgg_lr_reducer.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Freeze the layers which you don't want to train. Here I am freezing the first 5 layers.\n",
    "for layer in model.layers[:5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "#Adding custom Layers \n",
    "x = model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(1024, activation=\"relu\")(x)\n",
    "predictions = Dense(16, activation=\"softmax\")(x)\n",
    "\n",
    "# creating the final model \n",
    "model_final = Model(input = model.input, output = predictions)\n",
    "\n",
    "# compile the model \n",
    "model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "# Initiate the train and test generators with data Augumentation \n",
    "train_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "rescale = 1./255,\n",
    "horizontal_flip = True,\n",
    "fill_mode = \"nearest\",\n",
    "zoom_range = 0.3,\n",
    "width_shift_range = 0.3,\n",
    "height_shift_range=0.3,\n",
    "rotation_range=30)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "train_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "batch_size = batch_size, \n",
    "class_mode = \"categorical\")\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "validation_data_dir,\n",
    "target_size = (img_height, img_width),\n",
    "class_mode = \"categorical\")\n",
    "\n",
    "# Save the model according to the conditions  \n",
    "checkpoint = ModelCheckpoint(\"vgg16_1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "# Train the model \n",
    "model_final.fit_generator(\n",
    "train_generator,\n",
    "samples_per_epoch = nb_train_samples,\n",
    "epochs = epochs,\n",
    "validation_data = validation_generator,\n",
    "nb_val_samples = nb_validation_samples,\n",
    "callbacks = [checkpoint, early])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
