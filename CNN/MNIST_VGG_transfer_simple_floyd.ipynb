{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/keras-team/keras/blob/master/examples/mnist_transfer_cnn.py\n",
    "\n",
    "** Transfer learning toy example**\n",
    "\n",
    "- Train a simple convnet on the MNIST dataset the first 5 digits [0...4].\n",
    "- Freeze convolutional layers and fine-tune dense/output layers for the classification for the rest digits [5...9].\n",
    "\n",
    "Get to 99.8% test accuracy after 5 epochs for the first five digits classifier and 99.2% for the last five digits after transfer + fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function \n",
    "\n",
    "import datetime\n",
    "import keras \n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten \n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "now = datetime.datetime.now\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_classes = 5\n",
    "\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# input image dimensions \n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "# number of convolutional filters to use \n",
    "filters = 32\n",
    "\n",
    "# convolutional kernel size \n",
    "kernel_size = 3\n",
    "\n",
    "# size of pooling area for max pooling\n",
    "pool_size = 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    # tf backend has K.image_data_format() == 'channels_last'\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, train, test, num_classes):\n",
    "    \n",
    "    # change input data size so they match tensorflow requirements\n",
    "    x_train = train[0].reshape((train[0].shape[0],) + input_shape)\n",
    "    x_test = test[0].reshape((test[0].shape[0],) + input_shape)\n",
    "    \n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    # normalization like \n",
    "    x_train /= 255\n",
    "    x_test /= 255\n",
    "    \n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    \n",
    "    # one hot encode class vectors \n",
    "    y_train = keras.utils.to_categorical(train[1], num_classes)\n",
    "    y_test = keras.utils.to_categorical(test[1], num_classes)\n",
    "    \n",
    "    # compile for training\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adadelta', metrics = ['accuracy'])\n",
    "    \n",
    "    t = now()\n",
    "    model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, verbose = 1, validation_data = (x_test, y_test))\n",
    "    \n",
    "    print('Training time: %s' % (now() - t))\n",
    "    score = model.evaluate(x_test, y_test, verbose=0)\n",
    "    print('Test score:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30596, 28, 28)\n",
      "(28, 28)\n"
     ]
    }
   ],
   "source": [
    "# create two datasets one with digits below 5 and the other above\n",
    "\n",
    "x_train_lt5 = x_train[y_train < 5]\n",
    "y_train_lt5 = y_train[y_train < 5]\n",
    "x_test_lt5 = x_test[y_test < 5]\n",
    "y_test_lt5 = y_test[y_test < 5]\n",
    "\n",
    "x_train_gte5 = x_train[y_train >= 5]\n",
    "y_train_gte5 = y_train[y_train >= 5] - 5\n",
    "x_test_gte5 = x_test[y_test >= 5]\n",
    "y_test_gte5 = y_test[y_test >= 5] - 5\n",
    "\n",
    "print(x_train_lt5.shape)\n",
    "print(x_train_lt5[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define two groups of layers: feature(convolutions) and classification(dense)\n",
    "\n",
    "feature_layers = [Conv2D(filters, kernel_size, padding = 'valid', input_shape = input_shape),\n",
    "                 Activation('relu'),\n",
    "                 Conv2D(filters, kernel_size),\n",
    "                 Activation('relu'),\n",
    "                 MaxPooling2D(pool_size = pool_size),\n",
    "                 Dropout(0.25),\n",
    "                 Flatten()]\n",
    "\n",
    "\n",
    "classification_layers = [Dense(128),\n",
    "                         Activation('relu'),\n",
    "                         Dropout(0.5), \n",
    "                         Dense(num_classes), \n",
    "                         Activation('softmax')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the complete model\n",
    "\n",
    "model = Sequential(feature_layers + classification_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (30596, 28, 28, 1)\n",
      "30596 train samples\n",
      "5139 test samples\n",
      "Train on 30596 samples, validate on 5139 samples\n",
      "Epoch 1/5\n",
      "30596/30596 [==============================] - 53s 2ms/step - loss: 0.1744 - acc: 0.9437 - val_loss: 0.0274 - val_acc: 0.9903\n",
      "Epoch 2/5\n",
      "30596/30596 [==============================] - 55s 2ms/step - loss: 0.0477 - acc: 0.9854 - val_loss: 0.0174 - val_acc: 0.9942\n",
      "Epoch 3/5\n",
      "30596/30596 [==============================] - 57s 2ms/step - loss: 0.0321 - acc: 0.9908 - val_loss: 0.0098 - val_acc: 0.9969\n",
      "Epoch 4/5\n",
      "30596/30596 [==============================] - 51s 2ms/step - loss: 0.0262 - acc: 0.9923 - val_loss: 0.0179 - val_acc: 0.9946\n",
      "Epoch 5/5\n",
      "30596/30596 [==============================] - 52s 2ms/step - loss: 0.0215 - acc: 0.9934 - val_loss: 0.0085 - val_acc: 0.9973\n",
      "Training time: 0:04:28.141054\n",
      "Test score: 0.008491175898898667\n",
      "Test accuracy: 0.9972757345787118\n"
     ]
    }
   ],
   "source": [
    "# train model for 5-digit clasification [0...4]\n",
    "\n",
    "train_model(model, (x_train_lt5, y_train_lt5), (x_test_lt5, y_test_lt5), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# freeze feature layers and rebuild the model for a different classification task [5...9]\n",
    "\n",
    "for l in feature_layers:\n",
    "    l.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (29404, 28, 28, 1)\n",
      "29404 train samples\n",
      "4861 test samples\n",
      "Train on 29404 samples, validate on 4861 samples\n",
      "Epoch 1/5\n",
      "29404/29404 [==============================] - 21s 704us/step - loss: 0.2463 - acc: 0.9245 - val_loss: 0.0523 - val_acc: 0.9842\n",
      "Epoch 2/5\n",
      "29404/29404 [==============================] - 21s 718us/step - loss: 0.0801 - acc: 0.9764 - val_loss: 0.0385 - val_acc: 0.9872\n",
      "Epoch 3/5\n",
      "29404/29404 [==============================] - 19s 658us/step - loss: 0.0595 - acc: 0.9813 - val_loss: 0.0305 - val_acc: 0.9895\n",
      "Epoch 4/5\n",
      "29404/29404 [==============================] - 20s 665us/step - loss: 0.0504 - acc: 0.9838 - val_loss: 0.0318 - val_acc: 0.9893\n",
      "Epoch 5/5\n",
      "29404/29404 [==============================] - 19s 644us/step - loss: 0.0467 - acc: 0.9855 - val_loss: 0.0237 - val_acc: 0.9930\n",
      "Training time: 0:01:39.823435\n",
      "Test score: 0.023744944033462122\n",
      "Test accuracy: 0.9930055544126722\n"
     ]
    }
   ],
   "source": [
    "# tranfer - train dense layers for new classification task [5...9]. training using new task data \n",
    "\n",
    "train_model(model, (x_train_gte5, y_train_gte5), (x_test_gte5, y_test_gte5), num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transfer learning with VGG and MNIST\n",
    "\n",
    "- **retrain output dense layer only/ using VGG as feature extractor(bottleneck feature)** The output layer in a vgg16 is a softmax activation with 1000 categories. Remove this layer and replace it with a softmax layer of 10 categories. VGG architectue is changed in this case.\n",
    "\n",
    "- **freeze the weights of the first few layers** the first few layers capture universal features like curves and edges, which are also relevant to the new problem. VGG architectue is not changed in this case, but image size is **different** from which (224 x 224) VGG is trained on. Easier to retrain on the Dense layer Repalced VGG16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG16 as feature extractor & feed the features to a simple linear model\n",
    "\n",
    "### VGG16 has requirement on minimium input size, which cannot be met by MNIST input "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# retrain output dense layer only\n",
    "\n",
    "from keras.applications.vgg16 import VGG16\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11395072/11490434 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "58712064/58889256 [============================>.] - ETA: 0s_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, None, None, 3)     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, None, None, 64)    1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, None, None, 64)    36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, None, None, 64)    0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, None, None, 128)   73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, None, None, 128)   147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, None, None, 128)   0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, None, None, 256)   295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, None, None, 256)   590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, None, None, 256)   0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, None, None, 512)   1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, None, None, 512)   2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, None, None, 512)   0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load VGG16 model weights\n",
    "\n",
    "model = VGG16(weights = 'imagenet', include_top = False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# check which backend is used and reshape the data to match Keras' expectation \n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# VGG is trained on color images - repeat grayscale image over three color channels\n",
    "# np.repeat - if ``axis = 0`` it will be the first dimension and if ``axis = -1`` it will be the last dimension.\n",
    "# expand channels for both training and testing images \n",
    "\n",
    "train_color_channel = []\n",
    "for i in range(x_train.shape[0]):\n",
    "    train_color_channel.append(np.repeat(x_train[i], 3, axis= -1))\n",
    "    \n",
    "test_color_channel = []\n",
    "for i in range(x_test.shape[0]):\n",
    "    test_color_channel.append(np.repeat(x_test[i], 3, axis= -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 3)\n",
      "number of train samples:  60000\n",
      "number of test samples:  10000\n"
     ]
    }
   ],
   "source": [
    "# data normalization \n",
    "\n",
    "x_train = np.array(train_color_channel).astype('float32')\n",
    "x_test = np.array(test_color_channel).astype('float32')\n",
    "# normalization like \n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('number of train samples: ', x_train.shape[0])\n",
    "print('number of test samples: ', x_test.shape[0])\n",
    "\n",
    "\n",
    "# one hot encode class vectors\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extracting time: 0:00:34.852788\n",
      "(60000, 0, 0, 512)\n",
      "(10000, 0, 0, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([], shape=(60000, 0, 0, 512), dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract features from training and testing data using pretrained VGG16 without top \n",
    "\n",
    "now = datetime.datetime.now\n",
    "t = now()\n",
    "\n",
    "feature_train = model.predict(x_train)\n",
    "feature_test = model.predict(x_test)\n",
    "\n",
    "print('Feature extracting time: %s' % (now() - t))\n",
    "\n",
    "print(feature_train.shape)\n",
    "print(feature_test.shape)\n",
    "\n",
    "feature_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 5,130\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create the simple linear model that will be fed with extracted features \n",
    "\n",
    "new_model = Sequential()\n",
    "new_model.add(Dense(10, activation = 'softmax', input_shape = (feature_train.shape[-1], )))\n",
    "\n",
    "new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 0, 0, 512)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (60000,512)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5985fd33ff4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfeature_train_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfeature_test_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeature_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_train_reshaped\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (60000,512)"
     ]
    }
   ],
   "source": [
    "# rehshape features extracted for training and testing data to be suitable for model fitting\n",
    "print(feature_train.shape)\n",
    "\n",
    "feature_train_reshaped = feature_train.reshape(feature_train.shape[0], feature_train.shape[-1])\n",
    "feature_test_reshaped = feature_test.reshape(feature_test.shape[0], feature_test.shape[-1])\n",
    "print(feature_train_reshaped.shape)\n",
    "\n",
    "# model compile \n",
    "new_model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# model fitting \n",
    "# - x_train in the original model.fit has to be of (num_exampels, nrows, ncols, nchannels)\n",
    "# - now only the Dense layer is trained, x_trian's size should match this requiremnt \n",
    "\n",
    "new_model.fit(feature_train_reshaped, y_train, batch_size = batch_size, epochs = epochs, validation_data = (feature_test_reshaped, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save trained model weights \n",
    "\n",
    "new_model.save_weights('vgg_feature_extractor.h5')\n",
    "\n",
    "new_model.load_weights('vgg_feature_extractor.h5')\n",
    "\n",
    "new_model.evaluate(feature_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
